<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[大数据入门10-Yarn总结]]></title>
    <url>%2F2019%2F04%2F24%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%85%A5%E9%97%A810-Yarn%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[###]]></content>
  </entry>
  <entry>
    <title><![CDATA[大数据入门09-HDFS总结]]></title>
    <url>%2F2019%2F01%2F14%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%85%A5%E9%97%A809-HDFS%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[HDFS概述HDFS（Hadoop Distributed File System），它是一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。HDFS的使用场景：适合一次写入，多次读出的场景，且不支持文件的修改。适合用来做数据分析，并不适合用来做网盘应用。 HDFS优缺点优点 高容错性1)数据自动保存多个副本。它通过增加副本的形式，提高容错性2)某一个副本丢失以后，它可以自动恢复 适合处理大数据1)数据规模：能够处理数据规模达到GB、TB、甚至PB级别的数据2)文件规模：能够处理百万规模以上的文件数量，数量相当之大 可构建在廉价机器上，通过多副本机制，提高可靠性 缺点 不适合低延时数据访问，比如毫秒级的存储数据，是做不到的 无法高效的对大量小文件进行存储1)存储大量小文件的话，它会占用NameNode大量的内存来存储文件目录和块信息。这样是不可取的，因为NameNode的内存总是有限的2)小文件存储的寻址时间会超过读取时间，它违反了HDFS的设计目标 不支持并发写入、文件随机修改1)一个文件只能有一个写，不允许多个线程同时写2)仅支持数据append（追加），不支持文件的随机修改 HDFS组成架构1) NameNode（nn）：就是Master，它是一个主管、管理者。 管理HDFS的名称空间 配置副本策略 管理数据块（Block）映射信息 处理客户端读写请求 2) DataNode：就是Slave。NameNode下达命令，DataNode执行实际的操作 存储实际的数据块 执行数据块的读/写操作 3) Client：就是客户端 文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行上传 与NameNode交互，获取文件的位置信息 与DataNode交互，读取或者写入数据 Client提供一些命令来管理HDFS，比如NameNode格式化 Client可以通过一些命令来访问HDFS，比如对HDFS增删查改操作 4) Secondary NameNode：并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务 辅助NameNode，分担其工作量，比如定期合并Fsimage和Edits，并推送给NameNode 在紧急情况下，可辅助恢复NameNode HDFS的常用Shell操作基本语法bin/hadoop fs 具体命令 OR bin/hdfs dfs 具体命令dfs是fs的实现类 命令大全123456789101112131415161718192021222324252627282930313233343536[-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;] [-cat [-ignoreCrc] &lt;src&gt; ...] [-checksum &lt;src&gt; ...] [-chgrp [-R] GROUP PATH...] [-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...] [-chown [-R] [OWNER][:[GROUP]] PATH...] [-copyFromLocal [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;] [-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;] [-count [-q] &lt;path&gt; ...] [-cp [-f] [-p] &lt;src&gt; ... &lt;dst&gt;] [-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]] [-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;] [-df [-h] [&lt;path&gt; ...]] [-du [-s] [-h] &lt;path&gt; ...] [-expunge] [-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;] [-getfacl [-R] &lt;path&gt;] [-getmerge [-nl] &lt;src&gt; &lt;localdst&gt;] [-help [cmd ...]] [-ls [-d] [-h] [-R] [&lt;path&gt; ...]] [-mkdir [-p] &lt;path&gt; ...] [-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;] [-moveToLocal &lt;src&gt; &lt;localdst&gt;] [-mv &lt;src&gt; ... &lt;dst&gt;] [-put [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;] [-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;] [-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...] [-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...] [-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--set &lt;acl_spec&gt; &lt;path&gt;]] [-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...] [-stat [format] &lt;path&gt; ...] [-tail [-f] &lt;file&gt;] [-test -[defsz] &lt;path&gt;] [-text [-ignoreCrc] &lt;src&gt; ...] [-touchz &lt;path&gt; ...] [-usage [cmd ...]] 常用命令实操 -help：输出这个命令参数1hadoop fs -help rm 12345678-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ... : Delete all files that match the specified file pattern. Equivalent to the Unix command &quot;rm &lt;src&gt;&quot; -skipTrash option bypasses trash, if enabled, and immediately deletes &lt;src&gt; -f If the file does not exist, do not display a diagnostic message or modify the exit status to reflect an error. -[rR] Recursively deletes directories -ls: 显示目录信息1hadoop fs -ls / 123456Found 5 items-rw-r--r-- 3 root supergroup 1366 2019-02-15 15:33 /README.txtdrwxr-xr-x - root supergroup 0 2019-02-21 14:20 /directorydrwxr-xr-x - root supergroup 0 2019-02-18 10:09 /hadoopdrwxrwxr-x - root supergroup 0 2019-02-20 16:24 /tmpdrwxr-xr-x - root supergroup 0 2019-02-27 11:48 /user -mkdir：在HDFS上创建目录1hadoop fs -mkdir -p /sanguo/shuguo -moveFromLocal：从本地剪切粘贴到HDFS12touch kongming.txthadoop fs -moveFromLocal ./kongming.txt /sanguo/shuguo -appendToFile：追加一个文件到已经存在的文件末尾 123touch liubei.txtvim liubei.txthadoop fs -appendToFile liubei.txt /sanguo/shuguo/kongming.txt -cat：显示文件内容 1hadoop fs -cat /sanguo/shuguo/kongming.txt -chgrp 、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限 12hadoop fs -chmod 666 /sanguo/shuguo/kongming.txthadoop fs -chown atguigu:atguigu /sanguo/shuguo/kongming.txt -copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去 1hadoop fs -copyFromLocal NOTICE.txt / -copyToLocal：从HDFS拷贝到本地 1hadoop fs -copyToLocal /sanguo/shuguo/kongming.txt ./ -cp ：从HDFS的一个路径拷贝到HDFS的另一个路径 1hadoop fs -cp /sanguo/shuguo/kongming.txt /zhuge.txt -mv：在HDFS目录中移动文件1hadoop fs -mv /zhuge.txt /sanguo/shuguo/ -getmerge：合并下载多个文件，比如HDFS的目录 /user/atguigu/test下有多个文件:log.1, log.2,log.3,… 1hadoop fs -getmerge /sanguo/shuguo/* ./zaiyiqi.txt -get：等同于copyToLocal，就是从HDFS下载文件到本地 1hadoop fs -get /sanguo/shuguo/kongming.txt ./ -put：等同于copyFromLocal 1hadoop fs -put ./zaiyiqi.txt /user/ -rm：删除文件或文件夹 1hadoop fs -rm /user/zaiyiqi.txt -du统计文件夹的大小信息 1hadoop fs -du -s -h /user HDFS的数据流HDFS写数据流程 客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在 NameNode返回是否可以上传 客户端请求第一个 Block上传到哪几个DataNode服务器上 NameNode返回3个DataNode节点，分别为dn1,dn2,dn3 客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成 dn1、dn2、dn3逐级应答客户端 客户端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答 当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。（重复执行3-7步） HDFS读数据流程 客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址 挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据 DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验） 客户端以Packet为单位接收，先在本地缓存，然后写入目标文件 SecondaryNameNode和NameNode 第一阶段：NameNode启动 第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存 客户端对元数据进行增删改的请求 NameNode记录操作日志，更新滚动日志 NameNode在内存中对数据进行增删改 第二阶段：Secondary NameNode工作 Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果 Secondary NameNode请求执行CheckPoint NameNode滚动正在写的Edits日志 将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode Secondary NameNode加载编辑日志和镜像文件到内存，并合并 生成新的镜像文件fsimage.chkpoint 拷贝fsimage.chkpoint到NameNode NameNode将fsimage.chkpoint重新命名成fsimage DataNodeDataNode工作机制 一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳 DataNode启动后向NameNode注册，通过后，周期性（1小时）的向NameNode上报所有的块信息 心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用 集群运行中可以安全加入和退出一些机器]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据入门08-HIVE常用DDL操作]]></title>
    <url>%2F2019%2F01%2F05%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%85%A5%E9%97%A808-HIVE%E5%B8%B8%E7%94%A8DDL%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[创建数据库 创建一个数据库，数据库在 HDFS 上的默认存储路径是/user/hive/warehouse/*.db。 1create database db_hive; 避免要创建的数据库已经存在错误，增加 if not exists 判断。（标准写法） 1create database if not exists db_hive; 创建一个数据库，指定数据库在 HDFS 上存放的位置 1create database db_hive2 location '/db_hive2.db'; 修改数据库用户可以使用 ALTER DATABASE 命令为某个数据库的 DBPROPERTIES 设置键-值对属性值，来描述这个数据库的属性信息。数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置。 1alter database db_hive set dbproperties(&apos;createtime&apos;=&apos;20190101&apos;); 查询数据库查看数据库详情 显示数据库信息 1desc database db_hive; 显示数据库详细信息，extended 1desc database extended db_hive; 切换当前数据库1use db_hive; 删除数据库 删除空数据库 1drop database db_hive2; 如果删除的数据库不存在，最好采用 if exists 判断数据库是否存在 1drop database if exists db_hive2; 如果数据库不为空，可以采用 cascade 命令，强制删除 1drop database db_hive cascade; 创建表建表语法: 123456789CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name [(col_name data_type [COMMENT col_comment], ...)] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] [ROW FORMAT row_format] [STORED AS file_format] [LOCATION hdfs_path] CREATE TABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。 EXTERNAL 关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION），Hive 创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。 COMMENT：为表和列添加注释。 PARTITIONED BY 创建分区表 CLUSTERED BY 创建分桶表 SORTED BY 不常用 ROW FORMATDELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char][MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]| SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, …)]用户在建表的时候可以自定义 SerDe 或者使用自带的 SerDe。如果没有指定 ROW FORMAT 或者 ROW FORMAT DELIMITED，将会使用自带的 SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的 SerDe，Hive 通过 SerDe 确定表的具体的列的数据。SerDe是Serialize/Deserilize的简称，目的是用于序列化和反序列化。 STORED AS 指定存储文件类型常用的存储文件类型：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、RCFILE（列式存储格式文件）如果文件数据是纯文本，可以使用 STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE。 LOCATION ：指定表在 HDFS 上的存储位置. LIKE 允许用户复制现有的表结构，但是不复制数据. 管理表(内部表)默认创建的表都是所谓的管理表，有时也被称为内部表。因为这种表，Hive 会（或多或少地）控制着数据的生命周期。Hive 默认情况下会将这些表的数据存储在由配置项hive.metastore.warehouse.dir(例如，/user/hive/warehouse)所定义的目录的子目录下。 当我们删除一个管理表时，Hive 也会删除这个表中数据。管理表不适合和其他工具共享数据。 普通创建表 1234create table if not exists student2( id int, name string ) row format delimited fields terminated by '\t' stored as textfile location '/user/hive/warehouse/student2'; 根据查询结果创建表（查询的结果会添加到新创建的表中） 12create table if not exists student3 as select id, name from student; 根据已经存在的表结构创建表 1create table if not exists student4 like student; 查询表的类型 1desc formatted student2; 外部表 建表语句创建部门表 12345create external table if not exists default.dept( deptno int, dname string, loc int ) row format delimited fields terminated by '\t'; 创建员工表 12345678910create external table if not exists default.emp( empno int, ename string, job string, mgr int, hiredate string, sal double, comm double, deptno int) row format delimited fields terminated by '\t'; 向外部表中导入数据 12load data local inpath &apos;/opt/module/datas/dept.txt&apos; into table default.dept;load data local inpath &apos;/opt/module/datas/emp.txt&apos; into table default.emp; 查询结果12select * from emp; select * from dept; 查看表格式化数据 1desc formatted dept; 管理表与外部表的互相转换 修改内部表student2为外部表 1alter table student2 set tblproperties('EXTERNAL'='TRUE'); 查询表的类型 1desc formatted student2; 修改外部表student2为内部表 1alter table student2 set tblproperties('EXTERNAL'='FALSE'); 分区表分区表实际上就是对应一个 HDFS 文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。Hive 中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过 WHERE 子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多。因为表是外部表，所以 Hive 并非认为其完全拥有这份数据。删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉。 创建分区表 123456create table dept_partition( deptno int, dname string, loc string ) partitioned by (month string) row format delimited fields terminated by '\t'; 加载数据到分区表中 123load data local inpath &apos;/opt/module/datas/dept.txt&apos; into table default.dept_partition partition(month=&apos;201809&apos;);load data local inpath &apos;/opt/module/datas/dept.txt&apos; into table default.dept_partition partition(month=&apos;201808&apos;);load data local inpath &apos;/opt/module/datas/dept.txt&apos; into table default.dept_partition partition(month=&apos;201807&apos;); 查询分区表中数据 单分区查询1select * from dept_partition where month='201809'; 多分区联合查询12345select * from dept_partition where month='201809'unionselect * from dept_partition where month='201808'unionselect * from dept_partition where month='201807'; 增加分区 增加单个分区1alter table dept_partition add partition(month='201806'); 增加多个分区1alter table dept_partition add partition(month='201805') partition(month='201804'); 删除分区 删除单个分区1alter table dept_partition drop partition (month='201804'); 同时删除多个分区12alter table dept_partition drop partition (month='201805'), partition (month='201806'); 这里需要注意的是删除多个分区是以” , “ 分割，而增加多个分区是以空格分割。 查看分区表有多少分区 1show partitions dept_partition; 查看分区表结构 1desc formatted dept_partition; 修改表重命名表语法：1ALTER TABLE table_name RENAME TO new_table_name 增加/修改/替换列信息 语法 更新列1ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name] 增加和替换列 1ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...) ADD 是代表新增一字段，字段位置在所有列后面(partition 列前)，REPLACE 则是表示替换表中所有字段。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据入门07-Hive安装配置与用法]]></title>
    <url>%2F2019%2F01%2F02%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%85%A5%E9%97%A807-Hive%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%E4%B8%8E%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[在这篇文章中，我们会安装配置Hive,并且把Hive的元数据库配置在mysql上面。 Hive安装配置下载地址:https://mirrors.tuna.tsinghua.edu.cn/apache/hive/ 下载&amp;安装1wget https://mirrors.tuna.tsinghua.edu.cn/apache/hive/hive-2.3.4/apache-hive-2.3.4-bin.tar.gz 解压到/opt/module/目录1tar -zxvf tar -zxvf apache-hive-2.3.4-bin.tar.gz -C ../module/hive-2.3.4 配置环境变量1vim /etc/profile 插入以下内容123##HIVE_HOMEexport HIVE_HOME=/opt/module/hive-2.3.4export PATH=$PATH:$HIVE_HOME/bin 刷新环境1source /etc/profile 配置Hive切换到Hive/conf目录中1cp hive-env.sh.template hive-env.sh 在hive-env.sh文件中添加hadoop路径以及hive的conf路径12HADOOP_HOME=/opt/module/hadoop-2.7.7export HIVE_CONF_DIR=/opt/module/hive-2.3.4 安装MySQL数据库hive默认自带的derby数据库，但是在这种模式下，hive只能打开一个。所以我们把hive的元数据库搭建在MySQL上面 下载安装 下载地址：https://dev.mysql.com/downloads/mysql/5.7.html#downloads 首先卸载操作系统可能会自带的mariadb-libs1yum -y remove mariadb-libs 解压mysql rpm-bundle tar包12mkdir mysqltar -xvf mysql-5.7.25-1.el7.x86_64.rpm-bundle.tar -C mysql 开始安装mysq一定要按照下面的顺序来安装，否则会安装不成功：12345rpm -ivh mysql-community-common-5.7.25-1.el7.x86_64.rpmrpm -ivh mysql-community-libs-5.7.25-1.el7.x86_64.rpmrpm -ivh mysql-community-client-5.7.25-1.el7.x86_64.rpmrpm -ivh mysql-community-server-5.7.25-1.el7.x86_64.rpmrpm -ivh mysql-community-libs-compat-5.7.25-1.el7.x86_64.rpm 配置 启动mysql服务,并且设置为开机启动 12systemctl start mysqldsystemctl enable mysqld 查看root用户初始密码,并且登录MySQL 12grep password /var/log/mysqld.logmysql -uroot -p 修改MySQL的密码长度以及安全性要求因为MySQL有长度以及安全性的要求，所以需要对此作出修改 12set global validate_password_policy=0; set global validate_password_length=1; 修改root密码 1set password = password(&apos;123456&apos;); 设置远程登录权限 12grant all privileges on *.* to &apos;root&apos;@&apos;%&apos; identified by &apos;12345678&apos;;flush privileges; 到这里MySQL就配置完成了。 配置Hive的MySQL元数据库在hive/conf目录中新增hive-site.xml文件并且插入以下内容1cp hive-default.xml.template hive-site.xml 12345678910111213141516171819202122&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://hadoop101:3306/hive?createDatabaseIfNotExist=true&amp;amp;useSSL=false&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;Username to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123456&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt;&lt;/property&gt; 下载mysql-connector-java在 https://dev.mysql.com/downloads/connector/j/5.1.html 中下载mysql-connector-java，并且把该文件拉到../hive/lib目录中 初始化元数据库1schematool -dbType mysql -initSchema 在这里遇到一个比较坑的就是官方文档是在hive/bin目录执行上面的命令的，但我在执行这条命令之后就遇到这个报错：卡了好久，切换到hive/conf目录再执行这条命令居然成功了- - ，不知道是不是一个bug 启动Hive在启动hive之前需要先启动hadoop的dfs 启动dfs切换到hadoop的sbin目录1./start-dfs.sh 启动Hive1hive 一个简单的测试：1show databases; Hive 常见属性配置Hive 数据仓库位置配置 Default 数据仓库的最原始位置是在 hdfs 上的：/user/hive/warehouse 路径下 在仓库目录下，没有对默认的数据库 default 创建文件夹。如果某张表属于 default数据库，直接在数据仓库目录下创建一个文件夹。 修改 default 数据仓库原始位置（将 hive-default.xml.template 如下配置信息拷贝到hive-site.xml 文件中）12345&lt;property&gt;&lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;&lt;value&gt;/user/hive/warehouse&lt;/value&gt;&lt;description&gt;location of default database for the warehouse&lt;/description&gt;&lt;/property&gt; 查询后信息显示配置在 hive-site.xml 文件中添加如下配置信息，就可以实现显示当前数据库，以及查询表的头信息配置。12345678&lt;property&gt;&lt;name&gt;hive.cli.print.header&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hive.cli.print.current.db&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 重新启动 hive，对比配置前后差异 配置前： 配置后： Hive 运行日志信息配置Hive 的 log 默认存放在/tmp/atguigu/hive.log 目录下（当前用户名下）修改 hive 的 log 存放日志到/opt/module/hive/logs 修改/opt/module/hive/conf/hive-log4j.properties.template 文件名称为hive-log4j.properties 在 hive-log4j.properties 文件中修改 log 存放位置1hive.log.dir=/opt/module/hive/logs 到这里，Hive的安装配置就完成啦！]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据入门06-spark项目实战]]></title>
    <url>%2F2018%2F12%2F25%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%85%A5%E9%97%A806-spark%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[在这篇文章中，我们会用pyspark分析空气质量,并且把数据存入elasticsearch,用kibana进行可视化。 数据准备数据来源：http://stateair.net/web/historical/1/1.html 我们把数据下载到本地，然后上传到hdfs 1hadoop fs -put Beijing* /data 编写pyspark程序创建sparkSession1spark = SparkSession.builder.appName("project").getOrCreate() 加载数据123data2017 = spark.read.format("csv").option("header","true").option("inferSchema","true").load("/data/Beijing_2017_HourlyPM25_created20170803.csv").select("Year","Month","Day","Hour","Value","QC Name")data2016 = spark.read.format("csv").option("header","true").option("inferSchema","true").load("/data/Beijing_2016_HourlyPM25_created20170201.csv").select("Year","Month","Day","Hour","Value","QC Name")data2015 = spark.read.format("csv").option("header","true").option("inferSchema","true").load("/data/Beijing_2015_HourlyPM25_created20160201.csv").select("Year","Month","Day","Hour","Value","QC Name") 其中option(“inferSchema”,”true”)是用于自动推导数值类型的 编写自定义函数我们需要把value的值进行聚合，并且按照空气质量指数进行划分12345670-50 健康51-100 中等101-150 对敏感人群不健康151-200 不健康201-300 非常不健康301-500 危险&gt;500 爆表 自定义函数的一般流程1234567891011121314# 1.创建普通的python函数def toDate(s): return str(s)+'-'# 2.注册自定义函数from pyspark.sql.functions import udffrom pyspark.sql.types import StringType# 根据python的返回值类型定义好spark对应的数据类型# python函数中返回的是string，对应的pyspark是StringTypetoDateUDF=udf(toDate, StringType()) # 使用自定义函数df1.withColumn('color',toDateUDF('color')).show() 123456789101112131415161718def get_grade(value): if value &lt;= 50: return "健康" elif value &lt;= 100: return "中等" elif value &lt;= 150: return "对敏感人群不健康" elif value &lt;= 200: return "不健康" elif value &lt;= 300: return "非常不健康" elif value &lt;= 500: return "危险" elif value &gt; 500: return "爆表" else: return Nonegrade_function_udf = udf(get_grade, StringType()) 按空气质量指数进行分组聚合1234567group2017 = data2017.withColumn("Grade", grade_function_udf(data2017['Value'])).groupBy("Grade").count()group2016 = data2016.withColumn("Grade", grade_function_udf(data2016['Value'])).groupBy("Grade").count()group2015 = data2015.withColumn("Grade", grade_function_udf(data2015['Value'])).groupBy("Grade").count()result2017 = group2017.select("Grade", "count").withColumn("precent",group2017['count'] / data2017.count()*100)result2016 = group2016.select("Grade", "count").withColumn("precent",group2016['count'] / data2016.count()*100)result2015 = group2015.select("Grade", "count").withColumn("precent",group2015['count'] / data2015.count()*100) 数据写入到elasticsearch123result2017.selectExpr(&quot;Grade as grade&quot;, &quot;count&quot;, &quot;precent&quot;).write.format(&quot;org.elasticsearch.spark.sql&quot;).option(&quot;es.nodes&quot;,&quot;192.168.111.101:9200&quot;).mode(&quot;overwrite&quot;).save(&quot;weather2017/pm&quot;)result2016.selectExpr(&quot;Grade as grade&quot;, &quot;count&quot;, &quot;precent&quot;).write.format(&quot;org.elasticsearch.spark.sql&quot;).option(&quot;es.nodes&quot;,&quot;192.168.111.101:9200&quot;).mode(&quot;overwrite&quot;).save(&quot;weather2016/pm&quot;)result2015.selectExpr(&quot;Grade as grade&quot;, &quot;count&quot;, &quot;precent&quot;).write.format(&quot;org.elasticsearch.spark.sql&quot;).option(&quot;es.nodes&quot;,&quot;192.168.111.101:9200&quot;).mode(&quot;overwrite&quot;).save(&quot;weather2015/pm&quot;) 完整的代码在 https://github.com/dik111/pyspark-project/blob/master/13/wea.py Elasticsearch安装配置下载地址：https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.6.0.tar.gz 安装配置1tar -zxvf elasticsearch-6.6.0.tar.gz -C ../module/ 解压完成之后需要在elasticsearch目录中，修改配置文件/config/elasticsearch.yml添加以下内容12bootstrap.system_call_filter: falsenetwork.host: 0.0.0.0 启动elasticsearch在后台启动elasticsearch12cd /binnohup ./elasticsearch &amp; 这里需要注意的是elasticsearch不能用root用户启动，所以要切换到非root用户启动。 Kibana安装配置下载地址：https://artifacts.elastic.co/downloads/kibana/kibana-6.6.0-linux-x86_64.tar.gz 安装配置1tar -zxvf kibana-6.6.0-linux-x86_64.tar.gz -C ../module/ 解压完成之后需要在kibana目录中，修改/config/kibana.yml 文件添加以下内容：12server.host: &quot;0.0.0.0&quot;elasticsearch.hosts: [&quot;http://hadoop101:9200&quot;] 启动kibana12cd bin/nohup ./kibana &amp; 数据写入到elasticsearch下载elasticsearch-spark jar包下载地址：https://www.elastic.co/downloads/hadoop 写入数据12cd /opt/module/spark-2.3.2-bin-hadoop2.7/bin/./spark-submit --master yarn /home/dik/python-project/13/wea.py --jars /opt/module/elasticsearch-hadoop-6.6.0/dist/elasticsearch-spark-20_2.11-6.6.0.jar 如果提示：ClassNotFoundException Failed to find data source: org.elasticsearch.spark.sql.，则表示spark没有发现jar包，此时需重新编译pyspark：123cd /opt/module/spark-2.3.2-bin-hadoop2.7/pythonpython setup.py sdist pip install dist/*.tar.gz Kibana可视化 打开Mangement中的Index patterns 创建新的index pattern 然后在visualize中创建可视化条形图 Y轴： X轴： Split Series： Metrics： 最终完成效果：]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>pyspark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据入门05-spark运行模式]]></title>
    <url>%2F2018%2F12%2F21%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%85%A5%E9%97%A805-spark%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[本地模式本地模式是我们在IDE上面编写完程序，然后运行的一种模式。12345678./bin/spark-submit \ --class &lt;main-class&gt; \ --master &lt;master-url&gt; \ --deploy-mode &lt;deploy-mode&gt; \ --conf &lt;key&gt;=&lt;value&gt; \ ... # other options &lt;application-jar&gt; \ [application-arguments] 常用的 options : —class: 您的应用程序的入口点（例如。 org.apache.spark.examples.SparkPi) —master: 集群的 master URL (例如 spark://23.195.26.187:7077) —deploy-mode: 是在 worker 节点(cluster) 上还是在本地作为一个外部的客户端(client) 部署您的 driver(默认: client) † —conf: 按照 key=value 格式任意的 Spark 配置属性。对于包含空格的 value（值）使用引号包 “key=value” 起来。 application-jar: 包括您的应用以及所有依赖的一个打包的 Jar 的路径。该 URL 在您的集群上必须是全局可见的，例如，一个 hdfs:// path 或者一个 file:// 在所有节点是可见的。（对于 Python 应用，在 &lt;application-jar&gt; 的位置简单的传递一个 .py 文件而不是一个 JAR，并且可以用 —py-files 添加 Python .zip，.egg 或者 .py 文件到 search path（搜索路径）） application-arguments: 传递到您的 main class 的 main 方法的参数，如果有的话。 简单例子1./spark-submit --master local[2] --name spark-local /home/dik/python-project/03/spark0301.py Spark on yarn模式spark on yarn模式是我们工作中用的比较多的一种模式，其用法其实跟本地模式是比较类似的。spark作为客户端而已，他需要做的事情就是提交作业到yarn上去执行 配置spark on yarn 配置HADOOP_CONF_DIR 或者 YARN_CONF_DIR 1vim /spark-2.3.2-bin-hadoop2.7/conf/spark-env.sh 添加1HADOOP_CONF_DIR=/opt/module/hadoop-2.7.7/etc/hadoop 简单例子1./spark-submit --master yarn --name spark-local /home/dik/python-project/03/spark0301.py deploy modeyarn支持client和cluster模式：driver运行在哪里 client：提交作业的进程是不能停止的，否则作业就挂了 cluster：提交完作业，那么提交作业端就可以断开了，因为driver是运行在am里面的 开启历史日志监控spark的作业如果正在运行的话，我们可以在4040端口上面看，但是如果作业结束的话，我们便无法查看了。因此我们需要建立一个历史日志监控的系统。 在/spark-2.3.2-bin-hadoop2.7/conf目录中创建spark-defaults.conf文件 1cp spark-defaults.conf.template spark-defaults.conf 在当前安装Spark的节点上，进入到conf目录，在配置文件spark-defaults.conf添加下面的配置 12spark.eventLog.enabled true # 开启日志记录spark.eventLog.dir hdfs://hadoop101:9000/directory # 日志的保存位置 配置spark-env.sh 文件添加 1SPARK_HISTORY_OPTS=&quot;-Dspark.history.fs.logDirectory=hdfs://hadoop101:9000/directory&quot; 在/sbin目录下启动历史日志 1./start-history-server.sh]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>pyspark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据入门04-pyspark环境搭建]]></title>
    <url>%2F2018%2F12%2F19%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%85%A5%E9%97%A804-spark%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[安装Spark依赖的ScalaHadoop的安装请参考上面提到的博文，因为Spark依赖scala，所以在安装Spark之前，这里要先安装scala。在每个节点上都进行安装。 下载和解压缩Scala打开地址：https://www.scala-lang.org/download/ 配置scala环境变量 打开配置文件1vim /etc/profile 12export SCALA_HOME=/root/module/scala-2.12.8export PATH=$PATH:$SCALA_HOME/bin 保存之后刷新配置文件 1source /etc/profile 验证是否配置好 1scala -version 下载和解压缩Spark打开下载地址：http://spark.apache.org/downloads.html 选择清华源 配置spark环境变量 编辑/etc/profile文件，添加 123export SPARK_HOME=/root/module/spark-2.3.2-bin-hadoop2.7export PATH=$PATH:$SPARK_HOME/binexport PYSPARK_PYTHON=/home/dik/anaconda3/bin/python3.7 配置conf目录下的文件/root/module/spark-2.3.2-bin-hadoop2.7/conf目录下的文件进行配置 新建spark-env.h文件以spark为我们创建好的模板创建一个spark-env.h文件，命令是：1cp spark-env.sh.template spark-env.sh 编辑spark-env.h文件，在里面加入配置(具体路径以自己的为准)： 12345export SCALA_HOME=/root/module/scala-2.12.8export JAVA_HOME=/root/module/jdk1.8.0_191export HADOOP_HOME=/root/module/hadoop-2.7.7export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopexport SPARK_HOME=/root/module/spark-2.3.2-bin-hadoop2.7 新建slaves文件/root/module/spark-2.3.2-bin-hadoop2.7/conf目录下的文件进行配置 以spark为我们创建好的模板创建一个slaves文件，命令是：1cp slaves.template slaves 编辑slaves文件，里面的内容为：1spark001 启动和测试Spark集群启动Spark 切换到sbin目录 1cd /root/module/spark-2.3.2-bin-hadoop2.7/sbin 执行启动脚本： 1./start-all.sh 查看是否启动1jps 测试和使用Spark集群 访问Spark集群提供的URLhttp://spark001:8080/ 运行Spark提供的计算圆周率的示例程序进入到Spark的根目录 1cd .. 调用Spark自带的计算圆周率的Demo，执行下面的命令： 1./bin/spark-submit --class org.apache.spark.examples.SparkPi --master local examples/jars/spark-examples_2.11-2.3.2.jar 配置pyspark远程调试环境安装Anaconda3因为服务器自带的python版本是python2的，所以要安装python3,而Anaconda3是一个比较好的选择。 下载地址：https://repo.continuum.io/archive/Anaconda3-5.1.0-Linux-x86_64.sh 依赖安装bzip2： yum -y install bzip2 安装anaconda31bash Anaconda3-5.0.1-Linux-x86_64.sh 安装的过程比较简单，这里就不复述了。安装完成之后刷新配置文件1source ~/.bashrc 配置本地pycharm环境 安装py4j 1pip install py4j 配置pycharm 远程 配置远程的编译器 新建一个pyspark项目填入以下测试代码 1234567891011121314151617181920212223# -*- coding: UTF-8 -*-from pyspark import SparkConf, SparkContextdef my_map(): """ map(func) 将func函数作用到数据集的每一个元素上，生成一个新的分布式的数据集返回 word =&gt; (word,1) :return: """ data = [1, 2, 3, 4, 5] rdd1 = sc.parallelize(data) rdd2 = rdd1.map(lambda x: x * 2) print(rdd2.collect())if __name__ == '__main__': conf = SparkConf().setAppName('local[2]') sc = SparkContext(conf=conf) my_map() sc.stop() 配置 在里面加入对应的JAVA_HOME,PYTHONPATH,SPARK_HOME 运行代码如果没报错的话，那么就搭建成功了！]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>pyspark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据入门03-Hadoop完全分布式运行模式]]></title>
    <url>%2F2018%2F12%2F18%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%85%A5%E9%97%A803-Hadoop%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[虚拟机准备修改克隆主机的IP地址以及名字 我们打开克隆的3台centos7虚拟机，修改里面对应的IP地址为105,106，1071vim /etc/sysconfig/network-scripts/ifcfg-eth0 修改MAC地址在配置IP地址的文件中添加： 1MACADDR=XXXXXXXXXXXXXXXX 修改主机的名字，分别为105,106,107 1vim /etc/sysconfig/network 修改hosts配置文件，添加对应的105，106，107的IP地址 为三台虚拟机配置ssh分别在105，106，107上面创建ssh 1ssh-keygen -t rsa 然后敲（三个回车），就会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）进入到.ssh文件夹，将产生的公钥拷贝到目标机器上123ssh-copy-id hadoop105ssh-copy-id hadoop106ssh-copy-id hadoop107 编写xsync集群分发脚本在~/bin目录在创建xsync创建文件，文件内容如下：12touch xsyncvim xsync 12345678910111213141516171819202122232425#!/bin/bash#1 获取输入参数个数，如果没有参数，直接退出pcount=$#if((pcount==0)); thenecho no args;exit;fi#2 获取文件名称p1=$1fname=`basename $p1`echo fname=$fname#3 获取上级目录到绝对路径pdir=`cd -P $(dirname $p1); pwd`echo pdir=$pdir#4 获取当前用户名称user=`whoami`#5 循环for((host=103; host&lt;105; host++)); do echo ------------------- hadoop$host -------------- rsync -rvl $pdir/$fname $user@hadoop$host:$pdirdone 其中#5 循环部分可以改成对应的主机名称修改脚本 xsync 具有执行权限1chmod 777 xsync 集群配置 核心配置文件配置core-site.xml12[root@hadoop105 ~]# cd /opt/module/hadoop-2.7.7/etc/hadoop/[root@hadoop105 hadoop]# vim core-site.xml 把namenode节点改成105 HDFS配置文件 配置hadoop-env.sh 12[root@hadoop105 hadoop]# vim hadoop-env.shexport JAVA_HOME=/opt/module/jdk-11.0.1 配置hdfs-site.xml1[root@hadoop105 hadoop]# vim hdfs-site.xml 在该文件中编写如下配置12345&lt;!-- 指定Hadoop辅助名称节点主机配置 --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop107:50090&lt;/value&gt;&lt;/property&gt; YARN配置文件配置yarn-env.sh 12[root@hadoop105 hadoop]# vim yarn-env.shexport JAVA_HOME=/opt/module/jdk-11.0.1 配置yarn-site.xml1234567891011&lt;!-- Reducer获取数据的方式 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定YARN的ResourceManager的地址 --&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop106&lt;/value&gt;&lt;/property&gt; MapReduce配置文件配置mapred-env.sh12[root@hadoop105 hadoop]# vim mapred-env.shexport JAVA_HOME=/opt/module/jdk-11.0.1 配置mapred-site.xml12345678[root@hadoop105 hadoop]# cp mapred-site.xml.template mapred-site.xml[root@hadoop105 hadoop]# vim mapred-site.xml&lt;!-- 指定MR运行在Yarn上 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; 接着用xsync脚本把编写的文件同步到106,1071xsync /opt/module/hadoop-2.7.7/etc/hadoop/ 群起集群 格式化NameNode格式化之前，一定要先停止上次启动的所有namenode和datanode进程，然后再分别在105,106,107上删除data和log数据 12rm -rf data/ logs/bin/hdfs namenode -format 配置slaves 1234[root@hadoop105 hadoop-2.7.7]# vim etc/hadoop/slaveshadoop105hadoop106hadoop107 接着把它分发到106，1071~/bin/xsync /opt/module/hadoop-2.7.7/etc/hadoop/slaves 启动HDFS在hadoop105上启动：12sbin/start-dfs.sh#sbin/stop-dfs.sh (关闭) 分别在105，106，107上面用jps命令查看是否启动正确： 启动YARN在hadoop106上启动：12sbin/start-yarn.sh #sbin/stop-yarn.sh (关闭) 启动之后再在105，106，107上面用jps命令查看是否启动正确：最后我们登录 http://hadoop105:50070/explorer.html#/如果能成功打开，那么我们完全分布式集群就搭建成功了！ 集群基本测试我们上传一个小文件到集群上面用作测试:1bin/hdfs dfs -put README.txt /]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据入门02-Hadoop伪分布式运行模式]]></title>
    <url>%2F2018%2F12%2F17%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%85%A5%E9%97%A802-Hadoop%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[启动HDFS并运行MapReduce程序执行步骤 配置：hadoop-env.shLinux系统中获取JDK的安装路径,并且修改hadoop-env.sh文件中的JAVA_HOME：12echo $JAVA_HOMEvim /opt/module/hadoop-2.7.7/etc/hadoop/hadoop-env.sh 配置：core-site.xml1vim /opt/module/hadoop-2.7.7/etc/hadoop/core-site.xml 1234567891011&lt;!-- 指定HDFS中NameNode的地址 --&gt;&lt;property&gt;&lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop100:9000&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.7/data/tmp&lt;/value&gt;&lt;/property&gt; 配置：hdfs-site.xml1vim /opt/module/hadoop-2.7.7/etc/hadoop/hdfs-site.xml 12345&lt;!-- 指定HDFS副本的数量 --&gt;&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt; 这里因为是伪分布式模式，所以副本的数量设置为1。 启动集群 格式化NameNode（第一次启动时格式化，以后就不要总格式化） 12cd /opt/module/hadoop-2.7.7/bin/hdfs namenode -format 启动NameNode 1sbin/hadoop-daemon.sh start namenode 启动DataNode 1sbin/hadoop-daemon.sh start datanode 查看集群 查看是否启动成功 web端查看HDFS文件系统 注意：如果不能查看，看如下帖子处理http://www.cnblogs.com/zlslch/p/6604189.html 查看产生的Log日志1cd /opt/module/hadoop-2.7.7/logs/ 在企业中遇到Bug时，经常根据日志提示信息去分析问题、解决Bug。 操作集群 在HDFS文件系统上创建一个input文件夹 1bin/hdfs dfs -mkdir -p /user/dik/input 将测试文件内容上传到文件系统上 1bin/hdfs dfs -put wcinput/wc.input 运行MapReduce程序 1bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar wordcount /user/dik/input /user/dik/output 启动YARN并运行MapReduce程序执行步骤 配置yarn-env.sh 配置一下JAVA_HOME 123echo $JAVA_HOMEvim /opt/module/hadoop-2.7.7/etc/hadoop/hadoop-env.shexport JAVA_HOME=/opt/module/jdk-11.0.1 配置yarn-site.xml 1vim /opt/module/hadoop-2.7.7/etc/hadoop/yarn-site.xml 12345678910111213141516171819202122232425262728&lt;!-- Reducer获取数据的方式 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定YARN的ResourceManager的地址 --&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;&lt;value&gt;hadoop101&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;!-- 日志聚集功能使能 --&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;604800&lt;/value&gt; &lt;/property&gt; 配置：mapred-env.sh 配置一下JAVA_HOME 12vim /opt/module/hadoop-2.7.7/etc/hadoop/mapred-env.shexport JAVA_HOME=/opt/module/jdk-11.0.1 配置： (对mapred-site.xml.template重新命名为) mapred-site.xml 12mv mapred-site.xml.template mapred-site.xmlvim /opt/module/hadoop-2.7.7/etc/hadoop/ 12345678910111213141516&lt;!-- 指定MR运行在YARN上 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;!-- 历史服务器端地址 --&gt;&lt;property&gt;&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;&lt;value&gt;hadoop101:10020&lt;/value&gt;&lt;/property&gt;&lt;!-- 历史服务器web端地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;hadoop101:19888&lt;/value&gt;&lt;/property&gt; 启动集群启动前必须保证NameNode和DataNode已经启动 启动ResourceManager 1sbin/yarn-daemon.sh start resourcemanager 启动NodeManager 1sbin/yarn-daemon.sh start nodemanager 集群操作 YARN的浏览器页面查看 http://hadoop101:8088/cluster 删除文件系统上的output文件 1bin/hdfs dfs -rm -R /user/dik/outpu 执行MapReduce程序 1bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar wordcount /user/dik/input /user/dik/output 到这里一个简单的伪分布式运行模式就搭建成功啦！]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据入门01-Hadoop运行环境搭建]]></title>
    <url>%2F2018%2F12%2F09%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%85%A5%E9%97%A801-Hadoop%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[这篇文章主要介绍的是Hadoop运行环境搭建 虚拟机环境准备克隆虚拟机 修改克隆虚拟机的静态IP进入 /etc/sysconfig/network-scripts中12cd /etc/sysconfig/network-scriptsvim ifcfg-eth0 需要对这几项进行修改： 修改主机名12cd /etc/sysconfigsudo vim network 配置hosts文件 打开hosts文件 1cd /etc/hosts 配置 1234567192.168.111.100 hadoop100192.168.111.101 hadoop101192.168.111.102 hadoop102192.168.111.103 hadoop103192.168.111.104 hadoop104192.168.111.105 hadoop105192.168.111.106 hadoop106 关闭防火墙1systemctl stop firewalld.service 禁止防火墙开机启动1systemctl disable firewalld.service 在/opt目录下创建文件夹 在/opt目录下创建module、software文件夹,其中software文件夹用于软件包的存储，module文件夹用于软件的安装 12sudo mkdir modulesudo mkdir software 修改module、software文件夹的所有者 1sudo chown dik:dik module/ software/ 安装JDK以及配置环境变量卸载现有JDK 查询是否安装Java软件： 1rpm -qa | grep java 用Xshell工具将JDK导入到opt目录下面的software文件夹下面 在Linux系统下的opt目录中查看软件包是否导入成功 解压JDK到/opt/module目录下1tar -zxvf jdk-11.0.1_linux-x64_bin.tar.gz -C /opt/module/ 配置JDK环境变量 先获取JDK路径 12[dik@hadoop102 jdk-11.0.1]$ pwd/opt/module/jdk-11.0.1 打开/etc/profile文件 1sudo vim /etc/profile 在profile文件末尾添加JDK路径 12export JAVA_HOME=/opt/module/jdk-11.0.1export PATH=$PATH:$JAVA_HOME/bin 让修改后的文件生效 1source /etc/profile 测试JDK是否安装成功 安装Hadoop以及配置环境变量Hadoop下载地址：https://archive.apache.org/dist/hadoop/common/hadoop-2.7.7/也可以在官网直接下载。 导入Hadoop软件包用Xshell工具将hadoop-2.7.7.tar.gz导入到opt目录下面的software文件夹下面 安装Hadoop 进入到Hadoop安装包路径下 1cd /opt/software/ 解压安装文件到/opt/module下面 1tar -zxvf hadoop-2.7.7.tar.gz -C /opt/module/ 将Hadoop添加到环境变量 获取Hadoop安装路径 123[dik@hadoop102 module]$ cd hadoop-2.7.7/[dik@hadoop102 hadoop-2.7.7]$ pwd/opt/module/hadoop-2.7.7 打开/etc/profile文件,在profile文件末尾添加hadoop路径 1234##HADOOP_HOMEexport HADOOP_HOME=/opt/module/hadoop-2.7.7export PATH=$PATH:$HADOOP_HOME/binexport PATH=$PATH:$HADOOP_HOME/sbin 让修改后的文件生效 1source /etc/profile 测试是否安装成功 1234567[dik@hadoop102 hadoop-2.7.7]$ hadoop versionHadoop 2.7.7Subversion Unknown -r c1aad84bd27cd79c3d1a7dd58202a8c3ee1ed3acCompiled by stevel on 2018-07-18T22:47ZCompiled with protoc 2.5.0From source with checksum 792e15d20b12c74bd6f19a1fb886490This command was run using /opt/module/hadoop-2.7.7/share/hadoop/common/hadoop-common-2.7.7.jar Hadoop目录结构12345678910111213141516[dik@hadoop102 hadoop-2.7.7]$ lltotal 116drwxr-xr-x. 2 dik dik 194 Dec 6 23:56 bindrwxrwxr-x. 3 dik dik 17 Dec 7 22:50 datadrwxr-xr-x. 3 dik dik 20 Dec 6 23:56 etcdrwxr-xr-x. 2 dik dik 106 Dec 6 23:56 includedrwxrwxr-x. 2 dik dik 187 Dec 6 23:58 inputdrwxr-xr-x. 3 dik dik 20 Dec 6 23:56 libdrwxr-xr-x. 2 dik dik 239 Dec 6 23:56 libexec-rw-r--r--. 1 dik dik 86424 Dec 6 23:56 LICENSE.txtdrwxrwxr-x. 3 dik dik 4096 Dec 9 09:40 logs-rw-r--r--. 1 dik dik 14978 Dec 6 23:56 NOTICE.txtdrwxrwxr-x. 2 dik dik 88 Dec 6 23:58 output-rw-r--r--. 1 dik dik 1366 Dec 6 23:56 README.txtdrwxr-xr-x. 2 dik dik 4096 Dec 8 00:37 sbindrwxr-xr-x. 4 dik dik 31 Dec 6 23:57 share 重要目录（1）bin目录：存放对Hadoop相关服务（HDFS,YARN）进行操作的脚本（2）etc目录：Hadoop的配置文件目录，存放Hadoop的配置文件（3）lib目录：存放Hadoop的本地库（对数据进行压缩解压缩功能）（4）sbin目录：存放启动或停止Hadoop相关服务的脚本（5）share目录：存放Hadoop的依赖jar包、文档、和官方案例 本地模式运行Hadoop官方Grep案例 创建在hadoop-2.7.7文件下面创建一个input文件夹 1mkdir input 将Hadoop的xml配置文件复制到input 1cp etc/hadoop/*.xml input 执行share目录下的MapReduce程序 1bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar grep input output &apos;dfs[a-z.]+&apos; 查看输出结果 1cat output/* 到这里一个简单的Hadoop运行环境就搭建成功了！]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS224n笔记 自然语言处理与深度学习简介]]></title>
    <url>%2F2018%2F11%2F11%2FCS224n%E7%AC%94%E8%AE%B0-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[这是斯坦福CS224n的第一篇笔记，也是第一次系统地学习用深度学习来做自然语言处理。 什么是自然语言处理这是一门计算机科学、人工智能以及语言学的交叉学科。虽然语言只是人工智能的一部分（人工智能还包括计算机视觉等），但它是非常独特的一部分。这个星球上有许多生物拥有超过人类的视觉系统，但只有人类才拥有这么高级的语言。 自然语言处理的目标是让计算机处理或说“理解”自然语言，以完成有意义的任务，比如订机票购物或QA等。完全理解和表达语言是极其困难的，完美的语言理解等效于实现人工智能。 自然语言处理涉及的几个层次 作为输入一共有两个来源，语音与文本。所以第一级是语音识别和OCR或分词（事实上，跳过分词虽然理所当然地不能做句法分析，但字符级也可以直接做不少应用）。接下来是形态学，援引《统计自然语言处理》中的定义：形态学（morphology）：形态学（又称“词汇形态学”或“词法”）是语言学的一个分支，研究词的内部结构，包括屈折变化和构词法两个部分。由于词具有语音特征、句法特征和语义特征，形态学处于音位学、句法学和语义学的结合部位，所以形态学是每个语言学家都要关注的一门学科［Matthews,2000］。下面的是句法分析和语义分析，最后面的在中文中似乎翻译做“对话分析”，需要根据上文语境理解下文。 这门课主要关注画圈的三个部分，其中中间的两个是重中之重，虽然深度学习在语音识别上的发力最大。 自然语言处理应用一个小子集，从简单到复杂有： 拼写检查、关键词检索…… 文本挖掘（产品价格、日期、时间、地点、人名、公司名） 文本分类 机器翻译 客服系统 复杂对话系统 在工业界从搜索到广告投放、自动\辅助翻译、情感舆情分析、语音识别、聊天机器人\管家等等五花八门。 人类语言的特殊之处与信号处理、数据挖掘不同，自然语言的随机性小而目的性强；语言是用来传输有意义的信息的，这种传输连小孩子都能很快学会。人类语言是离散的、明确的符号系统。但又允许出现各种变种，比如颜文字，随意的错误拼写“I loooove it”。这种自由性可能是因为语言的可靠性（赘余性）。所以说语言文字绝对不是形式逻辑或传统AI的产物。 语言符号有多种形式（声音、手势、书写），在这些不同的形式中，其意义保持不变： 虽然人类语言是明确的符号系统，但符号传输到大脑的过程是通过连续的声学光学信号，大脑编码似乎是连续的激活值上的模式。另外巨大的词表也导致数据稀疏，不利于机器学习。这构成一种动机，是不是应该用连续的信号而不是离散的符号去处理语言。 什么是深度学习这是机器学习的一个子集。传统机器学习中，人类需要对专业问题理解非常透彻，才能手工设计特征。比如地名和机构名识别的特征模板： 然后把特征交给某个机器学习算法，比如线性分类器。机器为这些特征调整找到合适的权值，将误差优化到最小。在这个过程中一直在学习的其实是人类，而不是机器。机器仅仅做了一道数值优化的题目而已。下面这张图很好地展示了这个过程中的比例： 而深度学习是表示学习的一部分，用来学习原始输入的多层特征表示： 为什么NLP难人类语言是充满歧义的，不像编程语言那样明确。编程语言中有各种变量名，但人类语言中只有少数几个代词可以用，你得思考到底指代的是谁…… 人类语言的解读依赖于现实世界、常识以及上下文。由于说话速度书写速度阅读速度的限制，人类语言非常简练，省略了大量背景知识。 Deep NLP = Deep Learning + NLP将自然语言处理的思想与表示学习结合起来，用深度学习的手法解决NLP目标。这提高了许多方面的效果： 层次：语音、词汇、语法、语义 工具：词性标注、命名实体识别、句法\语义分析 应用：机器翻译、情感分析、客服系统、问答系统 深度学习的一个魅力之处是，它提供了一套“宇宙通用”的框架解决了各种问题。虽然工具就那么几个，但在各行各业都适用。 NLP表示层次：形态级别传统方法在形态级别的表示是词素： 深度学习中把词素也作为向量： 多个词素向量构成相同纬度语义更丰富的词向量。 NLP工具：句法分析 NLP语义层面的表示传统方法是手写大量的规则函数，叫做Lambda calculus： 在深度学习中，每个句子、短语和逻辑表述都是向量。神经网络负责它们的合并。 情感分析传统方法是请一两百个工人，手工搜集“情感极性词典”在词袋模型上做分类器。 深度学习复用了RNN来解决这个问题，它可以识别“反话”的情感极性： 注意这只是为了方便理解的示意图，并不是RNN的工作流程。私以为这张图放在这里不合适，可能会误导一部分人，以为神经网络就是这样的基于规则的“决策树”模型。 客服系统最著名的例子得数GMail的自动回复： 这是Neural Language Models的又一次成功应用，Neural Language Models是基于RNN的： 机器翻译传统方法在许多层级上做了尝试，词语、语法、语义之类。这类方法试图找到一种世界通用的“国际语”（Interlingua）来作为原文和译文的桥梁。 而Neural Machine Translation将原文映射为向量，由向量构建译文。也许可以说Neural Machine Translation的“国际语”是向量。 结论：所有层级的表示都是向量 这可能是因为向量是最灵活的形式，它的维度是自由的，它可以组合成矩阵，或者更高阶的Tensor。事实上，在实践的时候向量和矩阵没什么本质区别，经常看到为了效率或单纯的美观而pack成矩阵unroll成向量的操作。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[电力消耗预测]]></title>
    <url>%2F2018%2F10%2F30%2FXGBoost%E7%94%B5%E5%8A%9B%E6%B6%88%E8%80%97%E9%A2%84%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[项目背景企业用电需求预测一直是电力市场营销活动业务难点，大数据与云计算、人工智能等新技术相结合，给传统行业转型升级带来了新的机遇和思考。本项目主要用到开放扬中市高新区1000多家企业的历史用电量数据，通过模型算法预测该地区下一个月的每日总用电量。 数据采集本项目主要用到开放扬中市高新区1000多家企业的历史用电量数据，直接在官网下载csv文件即可。 数据处理从上面的图可以看到，原始数据主要由3个维度组成：user_id,record_date,power_consumption,分别对应企业ID，日期以及用电量 日期格式转换因为原数据的日期是strting类型，所以我们需要把它格式转换1234import pandas as pdf = open('电力数据power_AI.csv')df = pd.read_csv(f)df['record_date'] = pd.to_datetime(df['record_date']) 统计每日均值123base_df = df[[&apos;record_date&apos;,&apos;power_consumption&apos;]].groupby(by=&apos;record_date&apos;).agg(&apos;sum&apos;)base_df = base_df.reset_index()base_df.head() 特征工程因为数据只有3个维度，可能会造成欠拟合的情况，所以我们利用日期构建一些特征。 增加日期特征123456789101112base_df['dow'] = base_df['record_date'].apply(lambda x: x.dayofweek)base_df['doy'] = base_df['record_date'].apply(lambda x: x.dayofyear)base_df['day'] = base_df['record_date'].apply(lambda x: x.day)base_df['month'] = base_df['record_date'].apply(lambda x: x.month)base_df['year'] = base_df['record_date'].apply(lambda x: x.year)def map_season(month): month_dic = &#123;1:1, 2:1, 3:2, 4:2, 5:3, 6:3, 7:3, 8:3, 9:3, 10:4, 11:4, 12:1&#125; return month_dic[month]base_df['season'] = base_df['month'].apply(lambda x: map_season(x))base_df.head() 增加月度特征12345678base_df_stats = new_df = base_df[['power_consumption','year','month']].groupby(by=['year', 'month']).agg(['mean', 'std'])base_df_stats.columns = base_df_stats.columns.droplevel(0)base_df_stats = base_df_stats.reset_index()base_df_stats['1_m_mean'] = base_df_stats['mean'].shift(1)base_df_stats['2_m_mean'] = base_df_stats['mean'].shift(2)base_df_stats['1_m_std'] = base_df_stats['std'].shift(1)base_df_stats['2_m_std'] = base_df_stats['std'].shift(2)base_df_stats.head() 相关性检测12345678910111213import matplotlib.pyplot as plt%matplotlib inlineimport seaborn as snssns.set_style('darkgrid') #设定绘图的背景样式sns.set_palette('muted') #设定图表的颜色板plt.style.use('ggplot')from pylab import mplmpl.rcParams['font.sans-serif'] = ['FangSong'] # 指定默认字体mpl.rcParams['axes.unicode_minus'] = False # 解决保存图像是负号'-'显示为方块的问题corrmat = data_df[internal_chars].corr() #计算相关系数f , ax = plt.subplots(figsize = (10,6)) #设置图标尺寸大小plt.xticks(rotation = '0')sns.heatmap(corrmat, square=False, linewidths=.8, annot=True) #设置热力图参数 数据分析企业与用电量的关系可以看到ID为1416,174,175的企业用电量很大如果有必要可以把这3个企业单独分为1类做处理。 时间维度与用电量的关系 数据建模在这个项目中，我们主要用到XGboost这个算法模型 训练模型首先我们用模型默认的参数进行预测123456789101112131415df_finall = pd.read_excel('data_all.xlsx',sheet_name='V2')#加载数据from sklearn.cross_validation import KFold, train_test_splitfrom xgboost import XGBRegressorfrom sklearn import metricsfrom sklearn.cross_validation import KFold, train_test_splitX = df_finall.iloc[:,1:-1]X[['dow','doy','day','month','year','season']] = X[['dow','doy','day','month','year','season']]\.astype(str)y = df_finall.iloc[:,-1]X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.3)xgb1 = XGBRegressor()xgb1.fit(X_train,y_train)test_predictions = xgb1.predict(X_test)r2 = metrics.r2_score(y_test, test_predictions)r2 参数微调因为XGBoost自带的参数较多，所以我们采用网格搜索的方式对参数进行微调。 Step 1: 选择一组初始参数 1xgb1 = XGBRegressor(eta=0.01, num_boost_round=50, colsample_bytree=0.5, subsample=0.5,objective='reg:linear',seed=27) Step 2: 改变 max_depth 和 min_child_weight. 12345xgb_param_grid = &#123;'max_depth': list(range(4,9)), 'min_child_weight': list((1,3,6))&#125;grid = GridSearchCV(XGBRegressor(eta=0.01, num_boost_round=50, colsample_bytree=0.5, subsample=0.5,objective='reg:linear',seed=27), param_grid=xgb_param_grid, cv=5)grid.fit(X_train, y_train)grid.grid_scores_, grid.best_params_, grid.best_score_ 1234567891011121314151617([mean: 0.66566, std: 0.11042, params: &#123;&apos;max_depth&apos;: 4, &apos;min_child_weight&apos;: 1&#125;, mean: 0.65945, std: 0.10346, params: &#123;&apos;max_depth&apos;: 4, &apos;min_child_weight&apos;: 3&#125;, mean: 0.65507, std: 0.08334, params: &#123;&apos;max_depth&apos;: 4, &apos;min_child_weight&apos;: 6&#125;, mean: 0.67838, std: 0.09230, params: &#123;&apos;max_depth&apos;: 5, &apos;min_child_weight&apos;: 1&#125;, mean: 0.65974, std: 0.09295, params: &#123;&apos;max_depth&apos;: 5, &apos;min_child_weight&apos;: 3&#125;, mean: 0.65113, std: 0.08137, params: &#123;&apos;max_depth&apos;: 5, &apos;min_child_weight&apos;: 6&#125;, mean: 0.68250, std: 0.08758, params: &#123;&apos;max_depth&apos;: 6, &apos;min_child_weight&apos;: 1&#125;, mean: 0.66546, std: 0.10096, params: &#123;&apos;max_depth&apos;: 6, &apos;min_child_weight&apos;: 3&#125;, mean: 0.65293, std: 0.09142, params: &#123;&apos;max_depth&apos;: 6, &apos;min_child_weight&apos;: 6&#125;, mean: 0.67734, std: 0.08246, params: &#123;&apos;max_depth&apos;: 7, &apos;min_child_weight&apos;: 1&#125;, mean: 0.66691, std: 0.09677, params: &#123;&apos;max_depth&apos;: 7, &apos;min_child_weight&apos;: 3&#125;, mean: 0.66142, std: 0.08414, params: &#123;&apos;max_depth&apos;: 7, &apos;min_child_weight&apos;: 6&#125;, mean: 0.67381, std: 0.11030, params: &#123;&apos;max_depth&apos;: 8, &apos;min_child_weight&apos;: 1&#125;, mean: 0.67931, std: 0.09550, params: &#123;&apos;max_depth&apos;: 8, &apos;min_child_weight&apos;: 3&#125;, mean: 0.66090, std: 0.08821, params: &#123;&apos;max_depth&apos;: 8, &apos;min_child_weight&apos;: 6&#125;], &#123;&apos;max_depth&apos;: 6, &apos;min_child_weight&apos;: 1&#125;, 0.6825025931707269) 网格搜索发现的最佳结果:{‘max_depth’: 6, ‘min_child_weight’: 1} Step 3: 调节 gamma 降低模型过拟合风险.12345xgb_param_grid = &#123;'gamma':[ 0.01 * i for i in range(0,5)]&#125;grid = GridSearchCV(XGBRegressor(eta=0.01, num_boost_round=50, colsample_bytree=0.5, subsample=0.5,objective='reg:linear',seed=27,max_depth=7,min_child_weight=1), param_grid=xgb_param_grid, cv=5)grid.fit(X_train, y_train)grid.grid_scores_, grid.best_params_, grid.best_score_ 1234567([mean: 0.67462, std: 0.09512, params: &#123;&apos;gamma&apos;: 0.0&#125;, mean: 0.67462, std: 0.09512, params: &#123;&apos;gamma&apos;: 0.01&#125;, mean: 0.67462, std: 0.09512, params: &#123;&apos;gamma&apos;: 0.02&#125;, mean: 0.67462, std: 0.09512, params: &#123;&apos;gamma&apos;: 0.03&#125;, mean: 0.67462, std: 0.09512, params: &#123;&apos;gamma&apos;: 0.04&#125;], &#123;&apos;gamma&apos;: 0.0&#125;, 0.6746192263207629) 网格搜索发现的最佳结果:{‘gamma’: 0.0} Step 4: 调节 subsample 和 colsample_bytree 改变数据采样策略.1234567xgb_param_grid = &#123;'subsample':[ 0.1 * i for i in range(6,10)], 'colsample_bytree':[ 0.1 * i for i in range(6,10)]&#125;grid = GridSearchCV(XGBRegressor(eta=0.01, num_boost_round=50, colsample_bytree=0.5, subsample=0.5,objective='reg:linear',seed=27,max_depth=7, min_child_weight=1,gamma=0), param_grid=xgb_param_grid, cv=5)grid.fit(X_train, y_train)grid.grid_scores_, grid.best_params_, grid.best_score_ 123456789101112131415161718([mean: 0.67014, std: 0.13270, params: &#123;&apos;colsample_bytree&apos;: 0.6000000000000001, &apos;subsample&apos;: 0.6000000000000001&#125;, mean: 0.67302, std: 0.10802, params: &#123;&apos;colsample_bytree&apos;: 0.6000000000000001, &apos;subsample&apos;: 0.7000000000000001&#125;, mean: 0.66858, std: 0.11634, params: &#123;&apos;colsample_bytree&apos;: 0.6000000000000001, &apos;subsample&apos;: 0.8&#125;, mean: 0.67034, std: 0.11901, params: &#123;&apos;colsample_bytree&apos;: 0.6000000000000001, &apos;subsample&apos;: 0.9&#125;, mean: 0.66074, std: 0.12567, params: &#123;&apos;colsample_bytree&apos;: 0.7000000000000001, &apos;subsample&apos;: 0.6000000000000001&#125;, mean: 0.67163, std: 0.11620, params: &#123;&apos;colsample_bytree&apos;: 0.7000000000000001, &apos;subsample&apos;: 0.7000000000000001&#125;, mean: 0.67011, std: 0.11888, params: &#123;&apos;colsample_bytree&apos;: 0.7000000000000001, &apos;subsample&apos;: 0.8&#125;, mean: 0.67345, std: 0.10611, params: &#123;&apos;colsample_bytree&apos;: 0.7000000000000001, &apos;subsample&apos;: 0.9&#125;, mean: 0.67388, std: 0.11610, params: &#123;&apos;colsample_bytree&apos;: 0.8, &apos;subsample&apos;: 0.6000000000000001&#125;, mean: 0.67462, std: 0.10752, params: &#123;&apos;colsample_bytree&apos;: 0.8, &apos;subsample&apos;: 0.7000000000000001&#125;, mean: 0.68473, std: 0.10995, params: &#123;&apos;colsample_bytree&apos;: 0.8, &apos;subsample&apos;: 0.8&#125;, mean: 0.69456, std: 0.08956, params: &#123;&apos;colsample_bytree&apos;: 0.8, &apos;subsample&apos;: 0.9&#125;, mean: 0.66677, std: 0.11420, params: &#123;&apos;colsample_bytree&apos;: 0.9, &apos;subsample&apos;: 0.6000000000000001&#125;, mean: 0.66556, std: 0.10918, params: &#123;&apos;colsample_bytree&apos;: 0.9, &apos;subsample&apos;: 0.7000000000000001&#125;, mean: 0.67706, std: 0.11681, params: &#123;&apos;colsample_bytree&apos;: 0.9, &apos;subsample&apos;: 0.8&#125;, mean: 0.67529, std: 0.11429, params: &#123;&apos;colsample_bytree&apos;: 0.9, &apos;subsample&apos;: 0.9&#125;], &#123;&apos;colsample_bytree&apos;: 0.8, &apos;subsample&apos;: 0.9&#125;, 0.6945571414092301) 网格搜索发现的最佳结果:{‘colsample_bytree’: 0.8, ‘subsample’: 0.9} Step 5: 调节学习率 learning_rate.12345xgb_param_grid = &#123;'learning_rate':[0.6,0.5,0.4,0.3,0.2,0.1,0.01,0.001]&#125;grid = GridSearchCV(XGBRegressor( num_boost_round=50,objective='reg:linear',seed=27,max_depth=7, min_child_weight=1,gamma=0,colsample_bytree=0.8,subsample=0.9),param_grid=xgb_param_grid, cv=5)grid.fit(X_train, y_train)grid.grid_scores_, grid.best_params_, grid.best_score_ 12345678910([mean: 0.56345, std: 0.18185, params: &#123;&apos;learning_rate&apos;: 0.6&#125;, mean: 0.60402, std: 0.16560, params: &#123;&apos;learning_rate&apos;: 0.5&#125;, mean: 0.67215, std: 0.13258, params: &#123;&apos;learning_rate&apos;: 0.4&#125;, mean: 0.63116, std: 0.13087, params: &#123;&apos;learning_rate&apos;: 0.3&#125;, mean: 0.62305, std: 0.16146, params: &#123;&apos;learning_rate&apos;: 0.2&#125;, mean: 0.69456, std: 0.08956, params: &#123;&apos;learning_rate&apos;: 0.1&#125;, mean: -12.66985, std: 4.34214, params: &#123;&apos;learning_rate&apos;: 0.01&#125;, mean: -77.40167, std: 26.16110, params: &#123;&apos;learning_rate&apos;: 0.001&#125;], &#123;&apos;learning_rate&apos;: 0.1&#125;, 0.6945571414092301) 网格搜索发现的最佳结果:{‘learning_rate’: 0.1} step 6:调节n_estimators的棵数12345xgb_param_grid = &#123;'n_estimators':[50,100,200,300,400,500]&#125;grid = GridSearchCV(XGBRegressor( learning_rate=0.1,objective='reg:linear',seed=27,max_depth=7, min_child_weight=1,gamma=0,colsample_bytree=0.8,subsample=0.9),param_grid=xgb_param_grid, cv=5)grid.fit(X_train, y_train)grid.grid_scores_, grid.best_params_, grid.best_score_ 12345678([mean: 0.69015, std: 0.08067, params: &#123;&apos;n_estimators&apos;: 50&#125;, mean: 0.69456, std: 0.08956, params: &#123;&apos;n_estimators&apos;: 100&#125;, mean: 0.69296, std: 0.09306, params: &#123;&apos;n_estimators&apos;: 200&#125;, mean: 0.69270, std: 0.09327, params: &#123;&apos;n_estimators&apos;: 300&#125;, mean: 0.69269, std: 0.09334, params: &#123;&apos;n_estimators&apos;: 400&#125;, mean: 0.69270, std: 0.09335, params: &#123;&apos;n_estimators&apos;: 500&#125;], &#123;&apos;n_estimators&apos;: 100&#125;, 0.6945571414092301) 网格搜索发现的最佳结果:{‘n_estimators’: 100}模型最终的参数为：1234567XGBRegressor( learning_rate=0.1, objective=&apos;reg:linear&apos;, seed=27,max_depth=7, min_child_weight=1, gamma=0, colsample_bytree=0.8, subsample=0.9) 最终评分：123456xgb3 = XGBRegressor( learning_rate=0.1,objective='reg:linear',seed=27,max_depth=7, min_child_weight=1,gamma=0,colsample_bytree=0.8,subsample=0.9)xgb3.fit(X_train,y_train)test_predictions = xgb3.predict(X_test)r2 = metrics.r2_score(y_test, test_predictions)r2 10.7133043132603486]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-决策树&随机森林算法笔记与实战]]></title>
    <url>%2F2018%2F10%2F14%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%86%B3%E7%AD%96%E6%A0%91-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%AE%97%E6%B3%95%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[决策树 决策树：从根节点开始一步步走到叶子节点（决策） 所有的数据最终都会落到叶子节点，既可以做分类也可以做回归 树的组成 根节点：第一个选择点 非叶子节点与分支：中间过程 叶子节点：最终的决策结果 决策树的训练与测试 训练阶段：从给定的训练集构造出来一棵树（从跟节点开始选择特征， 如何进行特征切分） 测试阶段：根据构造出来的树模型从上到下去走一遍就好了 一旦构造好了决策树，那么分类或者预测任务就很简单了，只需要走一遍 就可以了，那么难点就在于如何构造出来一颗树，这就没那么容易了，需 要考虑的问题还有很多的！ 如何切分特征（选择节点） 问题：根节点的选择该用哪个特征呢？接下来呢？如何切分呢？ 想象一下：我们的目标应该是根节点就像一个老大似的能更好的切分数据 （分类的效果更好），根节点下面的节点自然就是二当家了。 目标：通过一种衡量标准，来计算通过不同特征进行分支选择后的分类 情况，找出来最好的那个当成根节点，以此类推。 衡量标准-熵 熵：熵是表示随机变量不确定性的度量 （解释：说白了就是物体内部的混乱程度，比如杂货市场里面什么都有 那肯定混乱呀，专卖店里面只卖一个牌子的那就稳定多啦） 公式：H(X)=- ∑ pi * logpi, i=1,2, … , n 一个栗子： A集合[1,1,1,1,1,1,1,1,2,2] B集合[1,2,3,4,5,6,7,8,9,1] 显然A集合的熵值要低，因为A里面只有两种类别，相对稳定一些 而B中类别太多了，熵值就会大很多。（在分类任务中我们希望通过 节点分支后数据类别的熵值大还是小呢？） 衡量标准-熵 熵：不确定性越大，得到的熵值也就越大当p=0或p=1时，H(p)=0,随机变量完全没有不确定性当p=0.5时，H(p)=1,此时随机变量的不确定性最大 信息增益：表示特征X使得类Y的不确定性减少的程度。 （分类后的专一性，希望分类后的结果是同类在一起） 决策树构造实例数据：14天打球情况特征：4种环境变化目标：构造决策树 划分方式：4种问题：谁当根节点呢？依据：信息增益 在历史数据中（14天）有9天打球，5天不打球，所以此时的熵应为： 4个特征逐一分析，先从outlook特征开始：Outlook = sunny时，熵值为0.971Outlook = overcast时，熵值为0Outlook = rainy时，熵值为0.971 根据数据统计，outlook取值分别为sunny,overcast,rainy的概率分别为： 5/14, 4/14, 5/14熵值计算：5/14 0.971 + 4/14 0 + 5/14 * 0.971 = 0.693（gain(temperature)=0.029 gain(humidity)=0.152 gain(windy)=0.048）信息增益：系统的熵值从原始的0.940下降到了0.693，增益为0.247同样的方式可以计算出其他特征的信息增益，那么我们选择最大的那个 就可以啦，相当于是遍历了一遍特征，找出来了大当家，然后再其余的 中继续通过信息增益找二当家！ 决策树算法 ID3：信息增益 C4.5：信息增益率 CART：使用GINI系数来当做衡量标准 GINI系数：和熵的衡量标准类似，计算方式不相同 决策树剪枝策略为什么要剪枝：决策树过拟合风险很大，理论上可以完全分得开数据 （想象一下，如果树足够庞大，每个叶子节点不就一个数据了嘛） 剪枝策略：预剪枝，后剪枝 预剪枝：边建立决策树边进行剪枝的操作（更实用）限制深度，叶子节点个数 叶子节点样本数，信息增益量等 后剪枝：当建立完决策树后来进行剪枝操作通过一定的衡量标准 随机森林随机森林是一种集成算法，是Bagging模型( bootstrap aggregation)的一种典型算法，随机指的是数据采样随机，特征选择随机。森林指的是很多个决策树并行放在一起。 构造树模型 由于二重随机性，使得每个树基本上都不会一样，最终的结果也会不一样 随机森林优势 它能够处理很高维度（feature很多）的数据，并且不用做特征选择 在训练完后，它能够给出哪些feature比较重要 容易做成并行化方法，速度比较快 可以进行可视化展示，便于分析]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask入门-搭建图书管理项目]]></title>
    <url>%2F2018%2F09%2F16%2FFlask%E5%85%A5%E9%97%A8-%E6%90%AD%E5%BB%BA%E5%9B%BE%E4%B9%A6%E7%AE%A1%E7%90%86%E9%A1%B9%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[在使用了一段django之后发现django太过于冗余，而简洁的Flask貌似更适合我。 搭建环境在这里推介大家使用pipenv这个虚拟环境 安装pipenv 1pip install pipenv 建立虚拟环境 1pipenv install 然后在这个文件夹内会创建2个文件 启动虚拟环境 1pipenv shell 安装flask 1pipenv install flask 打开pycharm，把刚刚搭建的虚拟环境配置到其中。 到这里，虚拟环境就已经配置完成啦！ 配置数据库在图管理这个项目中需要用到Mysql这个数据库 导入SQLALchemy扩展1234from flask_sqlalchemy import SQLAlchemy## 数据库配置：数据库地址/关闭自动跟踪修改app.config['SQLALCHEMY_DATABASE_URI'] = 'mysql://root:root@127.0.0.1/flask_books'app.config['SQLALCHEMY_TRACK_MODIFICATIONS']=False 需要注意的是在使用flask_sqlalchemy的时候可能会有ImportError: No module named MySQLdb 这里有个解决的办法就是到 https://www.lfd.uci.edu/~gohlke/pythonlibs/# 这里下载mysqlclient然后自行安装。 创建db对象,并配置参数123## 数据库配置：数据库地址/关闭自动跟踪修改app.config['SQLALCHEMY_DATABASE_URI'] = 'mysql://root:root@127.0.0.1/flask_books'app.config['SQLALCHEMY_TRACK_MODIFICATIONS']=False 创建数据库&amp;添加数据1234567891011121314151617db.drop_all()db.create_all()#生成数据au1 = Author(name='老王')au2 = Author(name='老惠')au3 = Author(name='老刘')db.session.add_all([au1,au2,au3])db.session.commit()bk1 = Book(name= '老王回忆录',author_id=au1.id)bk2 = Book(name='我读书少，你别骗我', author_id=au1.id)bk3 = Book(name='如何才能让自己更骚', author_id=au2.id)bk4 = Book(name='怎样征服美丽少女', author_id=au3.id)bk5 = Book(name='如何征服英俊少男', author_id=au3.id)db.session.add_all([bk1,bk2,bk3,bk4,bk5])db.session.commit() 添加书和作者模型1234567891011121314151617181920212223242526272829##a.模型继承ab.Model##b.__tablename__表名##c.db.Column:字段##d.relationship :关系引用class Author(db.Model): # 表名 __tablename__ = 'authors' #字段 id = db.Column(db.Integer,primary_key=True) name = db.Column(db.String(16),unique=True) #关系引用 books = db.relationship('Book',backref='author') def __repr__(self): return 'Author:%s' % self.name##书籍模型class Book(db.Model): __tablename__ = 'books' id = db.Column(db.Integer,primary_key=True) name = db.Column(db.String(16) , unique=True) author_id = db.Column(db.Integer,db.ForeignKey('authors.id')) def __repr__(self): return 'Book:%s %s' % (self.name,self.author_id) 使用模板显示数据库查询的数据查询所有的作者信息，让信息传递给模板123authors = Author.query.all()return render_template('books.html',authors = authors) 模板中按照格式，依次for循环作者和书籍（作者获取书籍，用的是关系引用）1234567891011121314&#123;#先遍历作者，然后在作者里面遍历书籍#&#125;&lt;ul&gt;&#123;% for author in authors %&#125;&lt;li&gt;&#123;&#123; author.name &#125;&#125;&lt;a href="&#123;&#123; url_for("delete_author",author_id = author.id) &#125;&#125;"&gt;删除&lt;/a&gt;&lt;/li&gt; &lt;ul&gt; &#123;% for book in author.books %&#125; &lt;li&gt;&#123;&#123; book.name &#125;&#125; &lt;a href="&#123;&#123; url_for("delete_book",book_id = book.id) &#125;&#125;"&gt;删除&lt;/a&gt;&lt;/li&gt; &#123;% else %&#125; &lt;li&gt;无&lt;/li&gt; &#123;% endfor %&#125; &lt;/ul&gt;&#123;% endfor %&#125; 使用WTF显示表单自定义表单类12345678from flask_wtf import FlaskFormfrom wtforms import StringField,SubmitField##自定义表单类class AuthorForm(FlaskForm): author = StringField('作者',validators=[DataRequired()]) book = StringField('书籍', validators=[DataRequired()]) submit = SubmitField('提交') 模板中显示1234567891011&lt;form method="post"&gt; &#123;&#123; form.csrf_token() &#125;&#125; &#123;&#123; form.author.label &#125;&#125;&#123;&#123; form.author &#125;&#125;&lt;br&gt; &#123;&#123; form.book.label &#125;&#125;&#123;&#123; form.book &#125;&#125;&lt;br&gt; &#123;&#123; form.submit &#125;&#125;&lt;br&gt; &#123;# 显示消息闪现的内容 #&#125; &#123;% for message in get_flashed_messages() %&#125; &#123;&#123; message &#125;&#125; &#123;% endfor %&#125;&lt;/form&gt; 消息闪现：secret_key/csrf_token12from flask import flashapp.secret_key='baidu' secret_key的作用 引用一段 Flask Web Development 中的内容:123SECRET_KEY 配置变量是通用密钥, 可在 Flask 和多个第三方扩展中使用. 如其名所示, 加密的强度取决于变量值的机密度. 不同的程序要使用不同的密钥, 而且要保证其他人不知道你所用的字符串. SECRET_KEY 的作用主要是提供一个值做各种 HASH, 我没有实际研究过源码, 不同框架和第三方库的功能也不尽相同, 我不能给出准确的答案, 但是主要的作用应该是在其加密过程中作为算法的一个参数(salt 或其他). 所以这个值的复杂度也就影响到了数据传输和存储时的复杂度. csrf_token()简单说来，使用它可以方便我们构建表单和验证表单，具体用法这里不做赘述详细说明以及用法请到 https://blog.csdn.net/baidu_35085676/article/details/78254954 实现相关的增删逻辑增加书籍123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657'''验证逻辑：1.调用WTF的函数实现验证2.验证通过获取数据3.判断作者是否存在4.如果作者存在，判断书籍是否存在，没有重复书籍，添加数据，如果重复就提示错误5.如果作者不存在，添加作者和书籍6.验证不通过就提示错误'''# 1.调用WTF的函数实现验证if author_form.validate_on_submit(): #2.验证通过获取数据 author_name = author_form.author.data book_name = author_form.book.data #3.判断作者是否存在 author = Author.query.filter_by(name = author_name).first() #4.如果作者存在 if author: #判断书籍是否存在，没有重复书籍，添加数据 book = Book.query.filter_by(name=book_name).first() if book: flash('已存在同名书籍') else: try: new_book = Book(name=book_name,author_id=author.id) db.session.add(new_book) db.session.commit( ) except Exception as e: print(e) flash('添加书籍失败') db.session.rollback() #回滚 else: #5.如果作者不存在，添加作者和书籍 try: new_author = Author(name=author_name) db.session.add(new_author) db.session.commit() new_book=Book(name=book_name,author_id=new_author.id) db.session.add(new_book) db.session.commit() except Exception as e: print(e) flash('添加作者和书籍失败') db.session.rollback()else: if request.method == 'POST': flash('参数不全')authors = Author.query.all()return render_template('books.html',authors = authors,form=author_form) 删除书籍删除书籍— 网页中删除 —点击需要发送书籍的ID—路由需要接受参数12345678910111213141516171819202122232425@app.route('/delete_book/&lt;book_id&gt;')def delete_book(book_id): # 1.查询数据库，是否有该ID的书，如果有就删除，没有就提示错误 book = Book.query.get(book_id) # 2.如果有就删除 if book: try: db.session.delete(book) db.session.commit() except Exception as e: print(e) flash('删除书籍出错') db.SessionExtension.rollback() else: # 3.没有提示错误 flash('书籍找不到') # redirect:重定向，需要传入网络/路由地址 # url_for('index'):需要传入视图函数名，返回该视图函数对应的路由地址 return redirect(url_for('index')) 删除作者删除作者具体的思路跟删除书籍差不多1234567891011121314151617181920212223242526@app.route('/delete_author/&lt;author_id&gt;')def delete_author(author_id): # 查询数据库，是否有该ID的作者，如果有就删除(先删书，再删作者)，没有就提示错误 # 1.查询数据库 author = Author.query.get(author_id) # 2.如果有就删除(先删书，再删作者) if author: try: #查询之后直接删除 Book.query.filter_by(author_id=author_id).delete() #删除作者 db.session.delete(author) db.session.commit() except Exception as e: print(e) flash('删除作者出错') db.session.rollback() else: flash('作者找不到') return redirect(url_for('index')) 到这里一个简单的图书管理网站就已经做好啦，完整的项目代码： https://github.com/dik111/Flask_book_project]]></content>
      <categories>
        <category>网站搭建</category>
      </categories>
      <tags>
        <tag>网站搭建</tag>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[django入门-搭建字数统计网站]]></title>
    <url>%2F2018%2F09%2F01%2Fdjango%E6%90%AD%E5%BB%BA%E5%AD%97%E6%95%B0%E7%BB%9F%E8%AE%A1%E7%BD%91%E7%AB%99%2F</url>
    <content type="text"><![CDATA[最近工作需要在数据分析系统中增加一些机器学习有关的工具，因为部门里面有些人不会python,为了方便大家使用，所以用django搭建一个网站。 安装django1pip install django 新建项目1django-admin startproject wordcount2 启动本地服务器进入新建的项目文件夹 1python manage.py runserver 如果你看到这个页面，就表示你的django安装成功啦！ 新建主页 在wordcount2文件夹中新建templates文件夹，用于存放html文件 在templates文件夹中新建home.html文件 12345678910111213141516&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;字数统计&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;字数统计&lt;/h1&gt; &lt;form &gt; &lt;textarea cols="100" rows="25" name="text"&gt;在此输入文本&lt;/textarea&gt; &lt;br&gt; &lt;input type="submit" value="统计"&gt; &lt;/form&gt; &lt;a href="about"&gt;关于本页&lt;/a&gt;&lt;/body&gt;&lt;/html&gt; 配置settings.py文件 在这里我们需要配置settings.py文件，用来告诉django，我们的html文件在哪。 新增function.py文件 在wordcount2文件夹中新增function.py文件，并且新增home函数。 12def home(request): return render(request,'home.html') render函数：1234567891011request: 是一个固定参数, 没什么好讲的。template_name: templates 中定义的文件, 要注意路径名. 比如'templates\polls\index.html', 参数就要写‘polls\index.html’context: 要传入文件中用于渲染呈现的数据, 默认是字典格式content_type: 生成的文档要使用的MIME 类型。默认为DEFAULT_CONTENT_TYPE 设置的值。status: http的响应代码,默认是200.using: 用于加载模板使用的模板引擎的名称。 配置在urls.py文件 在urls.py文件中，需要我们配置网站地址以及映射的函数。 在这里首页已经配置完成啦！ 增加统计结果网页当用户把需要统计的文字填入文字框中，并且按统计按钮之后，我们的页面需要跳转到统计结果的页面中。 配置home.html文件12345678910111213141516&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;字数统计&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;字数统计&lt;/h1&gt; &lt;form action="count"&gt; &lt;textarea cols="100" rows="25" name="text"&gt;在此输入文本&lt;/textarea&gt; &lt;br&gt; &lt;input type="submit" value="统计"&gt; &lt;/form&gt; &lt;a href="about"&gt;关于本页&lt;/a&gt;&lt;/body&gt;&lt;/html&gt; 在form标签中添加action=”count” 增加count.html文件 123456789101112&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;统计结果&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;统计结果，总字数为&#123;&#123;count&#125;&#125;字&lt;/h1&gt;&lt;h2&gt;你的文本&lt;/h2&gt;&#123;&#123;text&#125;&#125;&lt;/body&gt;&lt;/html&gt; 配置urls.py文件 在function中新增count函数1234def count(request): user_text = request.GET['text'] ##获取用户输入的文字 total_count = len(user_text) ##统计字数 return render(request,'count.html',&#123;'count':total_count,'text':user_text&#125;) 在这里需要注意的是我们通过render函数向html传递参数的时候需要以字典的形式传递。{‘count’:total_count,’text’:user_text} 如果没有出错的话，当你点击统计之后会跳转到这个页面 字数统计完整的count函数代码 123456789101112131415161718192021222324def count(request): user_text = request.GET['text'] user_text1 = request.GET['text'] user_text = re.findall('[\u4e00-\u9fa5]', user_text, re.S) ##用正则表达式去掉符号以及数字 total_count=len(user_text) word_dict = &#123;&#125; for word in user_text: if word not in word_dict: word_dict[word] = 1 else: word_dict[word] += 1 dataframe = pd.DataFrame(list(word_dict.keys()), columns=['字']) dataframe['频次'] = pd.DataFrame(list(word_dict.values())) dataframe.sort_values('频次', inplace=True, ascending=False) old_width = pd.get_option('display.max_colwidth') ##因为dataframe太长的话会显示不全，所以需要新增以下代码 pd.set_option('display.max_colwidth', -1) dataframe = dataframe.to_html(escape=False, index=False, sparsify=True, border=0, index_names=False, header=False) pd.set_option('display.max_colwidth', old_width) return render(request , 'count.html',&#123;'count':total_count , 'text':user_text1,'dict':dataframe&#125;) 上面的代码就是把用户输入的文字通过循环的方式统计字数，然后把字典转换成dataframe,传递给html。 完整的count.html 1234567891011121314151617&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;统计结果&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;统计结果，总字数为&#123;&#123;count&#125;&#125;字&lt;/h1&gt;&lt;h2&gt;你的文本&lt;/h2&gt;&#123;&#123;text&#125;&#125;&lt;br&gt;&#123;% autoescape off %&#125;&#123;&#123;dict&#125;&#125;&#123;% endautoescape %&#125;&lt;a href=".."&gt;回到主页&lt;/a&gt;&lt;/body&gt;&lt;/html&gt; 关于django自动转义Django的模板中会对HTML标签和JS等语法标签进行自动转义，原因显而易见，这样是为了安全。但是有的时候我们可能不希望这些HTML元素被转义，比如我们做一个内容管理系统，后台添加的文章中是经过修饰的，这些修饰可能是通过一个类似于FCKeditor编辑加注了HTML修饰符的文本，如果自动转义的话显示的就是保护HTML标签的源文件。为了在Django中关闭HTML的自动转义有两种方式，如果是一个单独的变量我们可以通过过滤器“|safe”的方式告诉Django这段代码是安全的不必转义。比如： 123&lt;p&gt;这行代表会被自动转义&lt;/p&gt;: &#123;&#123; data &#125;&#125;&lt;p&gt;这行代表不会被自动转义&lt;/p&gt;: &#123;&#123; data|safe &#125;&#125; 其中第二行我们关闭了Django的自动转义。我们还可以通过1&#123;%autoescape off%&#125; 的方式关闭整段代码的自动转义，比如下面这样： 1234&#123;% autoescape off %&#125; Hello &#123;&#123; name &#125;&#125;&#123;% endautoescape %&#125; 增加about页面我们需要新增一个about页面用于告诉用户，我们这个页面是用来干什么的,步骤与之前的都差不多。 增加about.html 123456789101112&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;关于本页&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;关于本页&lt;/h1&gt;&lt;h2&gt;本网站可以统计字数，并且会按频次降序排列&lt;/h2&gt;&lt;a href=".."&gt;回到主页&lt;/a&gt;&lt;/body&gt;&lt;/html&gt; 配置urls.py文件 配置function文件，新增about函数 12def about(request): return render(request ,'about.html') 在这里一个简单的字数统计网站就搭建好啦！完整项目代码:https://github.com/dik111/word_count]]></content>
      <categories>
        <category>网站搭建</category>
      </categories>
      <tags>
        <tag>网站搭建</tag>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[李宏毅机器学习笔记-4 Brief Introduction of Deep Learning；深度学习简介]]></title>
    <url>%2F2018%2F07%2F30%2F%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4-Brief-Introduction-of-Deep-Learning%EF%BC%9B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[Three Steps for Deep Learning Step 1: Neural Network 1212]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[李宏毅机器学习笔记-3 Classification,Logistic Regression]]></title>
    <url>%2F2018%2F07%2F26%2F%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3-Classification%E3%80%81Logistic-Regression%2F</url>
    <content type="text"><![CDATA[Classification分类算法在我们日常生活中随处可见： Credit ScoringInput: income, savings, profession, age, past financial history ……Output: accept or refuse Medical DiagnosisInput: current symptoms, age, gender, past medical history ……Output: which kind of diseases Handwritten character recognition Face recognitionInput: image of a faceoutput: person Example Application在这节课程中，李宏毅老师通过宝可梦的HP值，Attack值，SP Atk值，SP Def值，Speed值等一系列值来预测宝可梦属于哪种类型。 How to do Classification? Traning data for Classification Classification as Regression?Binary classification as exampleTraining: Class 1 means the target is 1; Class 2 means the target is -1Testing: closer to 1 → class 1; closer to -1 → class 2 Ideal Alternatives(理想的替代品) Function (Model): Loss function: Find the best funcion:Example:Perceptron , SVM在这里我们用线性回归算法进行分类，我们把接近-1的数据定义为class2,把接近1的数据定义为class1，以此作为分类，可以看到当数据远大于1时，绿色的直线慢慢往紫色的直线靠近了，因此在某种情况下，用线性回归进行分类可能会不太准确。Naive Bayes其中 mean μ 和 covariance matrix ∑ 是决定形状的两个因素，而不同的参数，会有不同的形状： Probability from Class 那如何找到这个Gaussion function呢？ Summery Probability Distribution Logistic RegressionThe steps of Logistic Regression Logistic Regression VS Linear Regression 为什么不能用Logistic Regression+Square Error Cross Entropy v.s. Square Error Discriminative（Logstic） v.s. Generative（Gaussion） Multi-class Classification Limitation of Logistic Regression]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[梯度下降算法实战]]></title>
    <url>%2F2018%2F06%2F29%2F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[在上一篇文章中，我们了解了梯度下降算法的原理，那么在这一篇文章中，我们将结合李宏毅机器学习入门的课后作业1，用python来实现梯度下降。 课后作业1内容&amp;数据集链接地址：https://ntumlta.github.io/2017fall-ml-hw1 载入数据1234567891011import pandas as pd import numpy as npimport matplotlib.pyplot as pltimport seaborn as snssns.set_style('darkgrid') #设定绘图的背景样式sns.set_palette('muted') #设定图表的颜色板from sklearn.linear_model import SGDRegressorfrom sklearn.cross_validation import train_test_splitfrom sklearn.model_selection import GridSearchCVdata = pd.read_csv('train.csv',encoding='big5')#用pandas读取csv文件 可以看到tranning set由这样的数据组成，我们的目标就是通过AMB_TEMP,CH4等一系列数据，预估出PM2.5的值。 数据清洗&amp;处理12345678910111213del data['datetime'] #删除无用指标datetimedel data['item']#删除无用指标itemdata.set_index(['obvservations'],inplace=True) data6 = pd.DataFrame()for i in range(0,240):#数据处理 data3 = data.iloc[i*18:(i+1)*18,:] data4 = data.iloc[(i+1)*18:(i+2)*18,:] data5 = pd.concat([data3,data4],axis=1) data6 = pd.concat([data5,data6],axis = 1)data7 = data6.Tdata8 = data7.dropna(how = 'all')del data8['RAINFALL']data9 = pd.DataFrame(data8,dtype='float') 经过一番处理之后，数据变成了这样的形式： 描述性分析12345678910111213plt.figure(figsize=(10, 6))plt.subplot(2, 2, 1)plt.title('AMB_TEMP')plt.scatter(data9['AMB_TEMP'],data9['PM2.5'])plt.subplot(2, 2, 2)plt.title('CH4')plt.scatter(data9['CH4'],data9['PM2.5'])plt.subplot(2, 2, 3)plt.title('CO')plt.scatter(data9['CO'],data9['PM2.5'])plt.subplot(2, 2, 4)plt.title('NMHC')plt.scatter(data9['NMHC'],data9['PM2.5']) 12345678910111213plt.figure(figsize=(10, 6))plt.subplot(2, 2, 1)plt.title('NO')plt.scatter(data9['NO'],data9['PM2.5'])plt.subplot(2, 2, 2)plt.title('NO2')plt.scatter(data9['NO2'],data9['PM2.5'])plt.subplot(2, 2, 3)plt.title('NOx')plt.scatter(data9['NOx'],data9['PM2.5'])plt.subplot(2, 2, 4)plt.title('O3')plt.scatter(data9['O3'],data9['PM2.5']) 12345678910111213plt.figure(figsize=(10, 6))plt.subplot(2, 2, 1)plt.title('WD_HR')plt.scatter(data9['WD_HR'],data9['PM2.5'])plt.subplot(2, 2, 2)plt.title('WIND_DIREC')plt.scatter(data9['WIND_DIREC'],data9['PM2.5'])plt.subplot(2, 2, 3)plt.title('WIND_SPEED')plt.scatter(data9['WIND_SPEED'],data9['PM2.5'])plt.subplot(2, 2, 4)plt.title('WS_HR')plt.scatter(data9['WS_HR'],data9['PM2.5']) 相关度计算12345internal_chars =['AMB_TEMP','CH4','CO','NMHC','NO','NO2','NOx','O3','PM10','PM2.5','RH','SO2','THC','WD_HR','WIND_DIREC','WIND_SPEED','WS_HR',]corrmat = data9[internal_chars].corr() #计算相关系数f , ax = plt.subplots(figsize = (20,12)) #设置图标尺寸大小plt.xticks(rotation = '0')sns.heatmap(corrmat, square=False, linewidths=.8, annot=True) #设置热力图参数 建模分析-预测PM2.5123456789X = data9[['PM10','NO2']]y = data9[['PM2.5']]scaler = StandardScaler()X = scaler.fit_transform(X)y = scaler.fit_transform(y)X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=1)clf = SGDRegressor(loss='epsilon_insensitive',alpha=0.01,penalty='l2',max_iter=10000,shuffle=True,n_iter=np.ceil(10**6/8622))clf.fit(X_train,y_train)clf.score(X,y) 其中loss = ‘epsilon_insensitive’ 表示用的最小二乘法，alpha = 0.01表示为初始的步长,max_iter=10000表示最大的迭代次数。最后模型的评分为0.62分，可能是数据量有点太少了，在以后的文章中，会继续把它优化！]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[李宏毅机器学习笔记-2 （Regression：Case Study ；回归：案例研究）]]></title>
    <url>%2F2018%2F06%2F26%2F%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2-%EF%BC%88Regression%EF%BC%9ACase-Study-%EF%BC%9B%E5%9B%9E%E5%BD%92%EF%BC%9A%E6%A1%88%E4%BE%8B%E7%A0%94%E7%A9%B6%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Regression-pokemons李老师在这一节课程开始介绍了用Regression，预测预测宝可梦（pokemons）进化后过的CP值（战斗力）。我们的目标是找出上帝函数’f’,通过imput一只宝可梦进化前的cp值，output他进化后的cp值。 Step1 选择Model在这里李老师建立以个Linear model: 123456 y = b + w * x它是infinite的……可能为f1: y = 10.0 + 9.0 ∙ x可能为f2: y = 9.8 + 9.2 ∙ x可能为f3: y = - 0.8 - 1.2 ∙ x…… 不同的b和w都会得到不同的f，而我们的目标就是找出一个最合适的f。 Step2 Goodness of function当我们将准备好的training data（已知10个宝可梦的进化情况），建立一个二维坐标轴。 通过上图可以看出，似乎有一个函数能够拟合这些坐标点，而这就是我们想要的，为了选出最契合的 f ，我们要建立一个Loss function L ，也就是函数的函数。 如果我们将 f 的 w 和 b 作为两轴，则在下图中每一点都代表一个 function f ，而颜色代表output的大小，也就代表该function f 参数的好坏。易理解，smallest点做对应的函数 f 就是我们想要的。 Step3 Best Function在lossfuction建立之后，我们需要通过这些这些lossfuction找到最合适的Best fuction,在这里，我们通过线性代数的基本公式来直接计算出最佳的w和b。 这种方法适用于单一特征的问题，但现实中，我们的问题往往是涉及到多个特征的，这时，则需要我们用更有效fuction,这里，李老师介绍了梯度下降法进行计算。首先我们需要随机选取一个w0,计算其斜率，如果为正，则减小w，如果为负，则增大w。而这有另一个问题，每次要增加或减少多少w值呢，有两个因素影响。第一，即微分值，如果微分值很大或很小，表示此处非常陡峭，那么证明距离最低点还有很远的距离，所以移动的距离就很大。第二个因素是我们事先自主定义的常数项 η 值，即步长。 经过无数次迭代之后，参数会经过无数次更新，最终到达一个最低值。而当我们有多个feature时，即不仅有w和b时，同样不会影响梯度下降过程，其原理为： How’s the results?经过上面的步骤后，我们得到了一个函数f，但我们发现并不是所有数据都能很好的拟合函数，这就会造成很大的预测不准的情况，通过Loss Function也能看出，最优解的值依然很大，测试数据的表现也不好，所以我们就要想办法优化。 很容易想到，刚刚我们用了一次方程作为model，二次方程会不会更好一些呢，三次方程、四次方程呢？于是我们做了以下实验，用同样的方法，放到多次方程中。 overfitting从上面的4幅图可以看出，虽然我们增加了函数次数后，可以使得training set 的error越来越小，但是test set的error并没有随着次数的增大而减小，甚至到5次时，结果大大超出了我们的预估，那么这种现象就叫做overfitting。 由此可见，fuction并不是越复杂越好，我们需要根据实际情况来选择合适的fuction，而对于overfitting的应对办法，我们一般采用以下方法来解决： 增加新特征，可以考虑加入进特征组合、高次特征，来增大假设空间; 尝试非线性模型，比如核SVM 、决策树、DNN等模型; 如果有正则项可以较小正则项参数 λ Boosting ,Boosting 往往会有较小的 Bias，比如 Gradient Boosting 等. underfitting有过拟合当然也少不了欠拟合了，对于欠拟合,我们一般采用以下方法解决： 增加新特征，可以考虑加入进特征组合、高次特征，来增大假设空间; 尝试非线性模型，比如核SVM 、决策树、DNN等模型; 如果有正则项可以较小正则项参数 λ. Boosting ,Boosting 往往会有较小的 Bias，比如 Gradient Boosting 等. 交叉检验当数据比较少是，留出一部分做交叉检验可能比较奢侈，还有只执行一次训练-测试来评估模型，会带有一些随机性，这些缺点都可以通过交叉检验克服，交叉检验对数据的划分如下： 交叉检验的步骤： 将数据分类训练集、验证集、测试集； 选择模型和训练参数； 使用训练集训练模型，在验证集中评估模型； 针对不同的模型，重复2）- 3）的过程； 选择最佳模型，使用训练集和验证集一起训练模型； 使用测试集来最终测评模型。 关于正则在模型的损失函数中引入正则项，可用来防止过拟合，于是得到的优化形式如下： w^*=argminwL(y,f(w,x))+λΩ(w)这里 Ω(w) 即为正则项， λ 则为正则项的参数，通常为 Lp 的形式，即： Ω(w)=||w||^p实际应用中比较多的是 L1 与 L2 正则，L1 正则是 L0 正则的凸近似，这里 L0 正则即为权重参数 w 中值为 0 的个数，但是求解 L0 正则是个NP 难题，所以往往使用 L1 正则来近似 L0 , 来使得某些特征权重为 0 ，这样便得到了稀疏的的权重参数 w。 在下一篇的文章我会结合李宏毅老师的课后作业，用python实现梯度下降算法，敬请期待！ 相关参考：https://blog.csdn.net/soulmeetliang/article/details/72619885https://www.cnblogs.com/ooon/p/5715918.html]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas入门简介]]></title>
    <url>%2F2017%2F10%2F25%2FPandas%E5%85%A5%E9%97%A8%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[Pandas简介pandas 是基于NumPy 的一种工具，该工具是为了解决数据分析任务而创建的。pandas提供了大量能使我们快速便捷地处理数据的函数和方法。你很快就会发现，它是使Python成为强大而高效的数据分析环境的重要因素之一。pandas主要使用的是两个数据结构Series和Dataframe,我们先导入它们以及相关模块： 123# -*- coding:utf-8 -*-import numpy as npfrom pandas import Series, DataFrame Pandas数据结构：Series一般来说，Series可以被认为是一维数组，Series与一维数组最主要的区别是Series具有索引（index），可以与另一个程序中常见的数据结构联系起来。 Series的创建创建Series的基本格式是s = Series(data, index=index, name=name)，下面给出几个创建Series的例子。123456a = np.random.randn(5)print （"a is an array:"）print （a）s = Series(a)print （"s is a Series:"）print （s） 123456789a is an array:[-1.24962807 -0.85316907 0.13032511 -0.19088881 0.40475505]s is a Series:0 -1.2496281 -0.8531692 0.1303253 -0.1908894 0.404755dtype: float64 在创建Series时可以添加index，而且可以使用Series.index查看具体的index，但是需要注意的一点是，当从数组创建Series时，若指定index，那么index长度要和data的长度一致：123s = Series(np.random.randn(5), index = ['a' , 'b' , 'c' , 'd' , 'e'])print(s)print(s.index) 1234567a -0.566972b -0.426072c 0.787193d 0.526550e -1.271557dtype: float64Index([&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;e&apos;], dtype=&apos;object&apos;) Series还可以从字典（dict）创建：123456d = &#123;'a' : 0. , 'b' : 1. , 'c' : 2&#125;print("d is a dict:")print(d)s = Series(d)print( "s is a Series:")print(s) 1234567d is a dict:&#123;&apos;a&apos;: 0.0, &apos;b&apos;: 1.0, &apos;c&apos;: 2&#125;s is a Series:a 0.0b 1.0c 2.0dtype: float64 #]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>数据分析</tag>
      </tags>
  </entry>
</search>
